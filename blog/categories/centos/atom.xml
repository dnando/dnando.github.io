<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Centos | Nando @ Aria Media]]></title>
  <link href="http://dnando.github.io/blog/categories/centos/atom.xml" rel="self"/>
  <link href="http://dnando.github.io/"/>
  <updated>2015-01-15T18:19:59+01:00</updated>
  <id>http://dnando.github.io/</id>
  <author>
    <name><![CDATA[Nando @ Aria Media]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Using Nginx With ColdFusion or Railo]]></title>
    <link href="http://dnando.github.io/blog/2015/01/05/advantages-of-nginx/"/>
    <updated>2015-01-05T20:26:47+01:00</updated>
    <id>http://dnando.github.io/blog/2015/01/05/advantages-of-nginx</id>
    <content type="html"><![CDATA[<p>Over the past few weeks I&rsquo;ve installed and experimented with the Nginx webserver. At this point it seems to me there are some significant advantages to using Nginx over Apache, advantages that aren&rsquo;t often highlighted, specifically when using either ColdFusion or Railo as your application server.</p>

<p>First up, it was very easy to install, start, and enable so that it always starts at boot time. These are the commands I used for CentOS 7, adapt as necessary for your operating system:</p>

<pre><code>rpm -Uvh http://nginx.org/packages/centos/7/noarch/RPMS/nginx-release-centos-7-0.el7.ngx.noarch.rpm
yum install nginx
systemctl start nginx.service
systemctl enable nginx.service
</code></pre>

<p>I could then browse to the server ip and see the Nginx welcome page.</p>

<p>It then took me a bit of time for me to grok how the configuration setup works. However, compared to Apache&rsquo;s massive conf file, the fact that I could easily see the entire main configuration in a single screen immediately put me at ease. Basically, Nginx has a main config file, located at /etc/nginx/nginx.conf on my CentOS install, which can be extended with includes. Before I started messing with it, the main conf file looked something like this:</p>

<pre><code class="">user  nginx;
worker_processes  1;
error_log  /var/log/nginx/error.log warn;
pid        /var/run/nginx.pid;

events {
    worker_connections  1024;
}

http {
    include       /etc/nginx/mime.types;
    default_type  application/octet-stream;

    log_format  main  '$remote_addr - $remote_user [$time_local] "$request" '
                      '$status $body_bytes_sent "$http_referer" '
                      '"$http_user_agent" "$http_x_forwarded_for"';

    access_log  /var/log/nginx/access.log  main;

    include /etc/nginx/conf.d/*.conf;
}
</code></pre>

<p>&hellip; and the default site conf file, which is picked up by the include at the bottom of the main conf file above, looked like this:</p>

<pre><code>server {
    listen       80;
    server_name  localhost;

    #charset koi8-r;
    #access_log  /var/log/nginx/log/host.access.log  main;

    location / {
        root   /usr/share/nginx/html;
        index  index.html index.htm;
    }

    #error_page  404              /404.html;

    # redirect server error pages to the static page /50x.html
    #
    error_page   500 502 503 504  /50x.html;
    location = /50x.html {
        root   /usr/share/nginx/html;
    }

    # proxy the PHP scripts to Apache listening on 127.0.0.1:80
    #
    #location ~ \.php$ {
    #    proxy_pass   http://127.0.0.1;
    #}

    # pass the PHP scripts to FastCGI server listening on 127.0.0.1:9000
    #
    #location ~ \.php$ {
    #    root           html;
    #    fastcgi_pass   127.0.0.1:9000;
    #    fastcgi_index  index.php;
    #    fastcgi_param  SCRIPT_FILENAME  /scripts$fastcgi_script_name;
    #    include        fastcgi_params;
    #}

    # deny access to .htaccess files, if Apache's document root
    # concurs with nginx's one
    #
    #location ~ /\.ht {
    #    deny  all;
    #}
}
</code></pre>

<p>Note the asterik syntax in the include at the bottom of the main conf file:
<code>include /etc/nginx/conf.d/*.conf;</code>.
Effectively, any .conf file you drop in that directory becomes part of the configuration ( specifically of the http{} block ).</p>

<p>Note also that there are semicolons at the end of every setting. If you see errors when reloading conf files using the command <code>nginx -s reload</code>, look for missing semicolons.</p>

<p>In a nutshell, Nginx&rsquo;s configuration is a simple nested structure, like this:</p>

<pre><code>nginx global settings

events {

}

http {

    server {
        listen 80;   # port to listen on
        server_name domain.com *.domain.com domain.net *.domain.net;
        root /var/www/domain;  #absolute path to root directory of website files

        location /downloads {
            # specific settings for a file system location relative to the root
            # which in this example would be /var/www/domain/downloads
        }

        location ~* \.(?:ico|css|js|gif|jpe?g|png)$ {
            # specific settings for files ending in suffixes .ico, .css, .js, gif, etc
            # defined using a regex, under the root directory defined in the server block
        }

        location / {
            # general settings for all files under/var/www/domain in this example
            # in this example only for those files that do not match the more specific location directives above
        }

        location = / {
            # this directive is triggered when only the domain is specified in the url
            # in this example for the url http://example.com
        }

    }

    server {
        # configuration for another virtual server, 
        # defined by a unique server_name or listen directive
    }

}
</code></pre>

<p>Each server block defines a virtual server, similar to a virtual host in Apache. The <code>server_name</code> directive takes a space delimited list of domains, which can include an asterisk as a wildcard character, replacing the first or last part of a name, to specify for instance any subdomain or TLD. Note that a directive like <code>server_name example.com *.example.com</code> can be also written with the shorthand expression <code>server_name .example.com</code></p>

<p>Nginx selects and processes locations within a server block using the most specific match, with regexes taking precedence, followed by the longest matching prefix. So in the above example, Nginx would process a file named myImage.jpg using the second location directive, a file in the downloads directory named myInfo.pdf using the first location directive, a file named index.htm ( or index.cfm ) using the third location directive, and a request to the root of the domain, say to <a href="http://domain.com/,">http://domain.com/,</a> with the last location directive, where the addition of the &ldquo;=&rdquo; sign specifies an exact match.</p>

<p>See the <a href="http://nginx.com/resources/admin-guide/web-server/">Nginx Web Server configuration guide</a> for more details.</p>

<p>To use Nginx with either ColdFusion or Railo, the <code>proxy_pass</code> directive is used, as in the following example:</p>

<pre><code>server {
    listen 80;   # port to listen on
    server_name domain.com *.domain.com domain.net *.domain.net;
    root /var/www/domain;  #absolute path to root directory of website files

    location /downloads {
        # specific settings for a file system location relative to the root
        # which in this example would be /var/www/domain/downloads
    }

    location ~* \.(?:ico|css|js|gif|jpe?g|png)$ {
        # specific settings for files ending in suffixes .ico, .css, .js, gif, etc
        # defined using a regex, under the root defined in the server block
    }

    location / {
        proxy_pass  http://127.0.0.1:8500/domain/;
    }

}
</code></pre>

<p>In the above example, Nginx passes the request to ColdFusion, running on the same server, via port 8500. It gets the response back from CF, immediately terminates the connection to CF so it is available to handle the next request, and then serves the static files using the settings in the second location directive, along with the rendered html from ColdFusion to the user that made the request. Note that this configuration assumes that you&rsquo;ve <a href="http://dnando.github.io/blog/2014/12/04/change-location-of-cf11-webroot/">changed the location of the CF webroot</a>.</p>

<p>The first advantage here is that there is no connector needed between ACF or Railo and the webserver. We don&rsquo;t need to install a connector as part of the initial server setup, and we avoid any bugs or configuration problems that might crop up with these connectors. We also avoid the necessity to sometimes manually reinstall connectors when updating ACF/Railo. All that is needed is to add the proxy_pass directive and reload the configuration using <code>nginx -s reload</code>.</p>

<p>The second advantage is that as long as you have a firewall in place that only allows public access via ports 80, ( plus maybe port 443, and whatever port you are using for ssh access), your ACF or Railo server is secured, simply because without a connector between the webserver and your application server, there is no direct access via port 80 or port 443 to your application server. In our example above, anyone browsing to domain.com/CFIDE/administrator/ will get a 404 error. The CFIDE directory isn&rsquo;t in the root specified. Port 8500 is closed, so ip:8500/CFIDE/administrator/ won&rsquo;t return a response. The ACF admin directory is not contained in the default site Nginx root, so ip:/CFIDE/administrator/, for instance, will simply return a 404.</p>

<p>How do you gain access to the admin areas of ColdFusion or Railo on a remote server if ports 8500 or port 8888 are closed? Simple. Use <a href="http://dnando.github.io/blog/2014/11/04/ssh-tunneling-coldfusion-lockdown-technique/">SSH Tunneling</a>.</p>

<p>In the event that the CFIDE, WEB-INF, or railo-context directories are exposed through a particular configuration setup in a particular server block, there is a simple way to handle this using location directives that can be set globally or included in each server block necessary, for example:</p>

<pre><code>location ~ /WEB-INF/  { access_log off; log_not_found off; deny all; }
location ~ /META-INF/  { access_log off; log_not_found off; deny all; }
</code></pre>

<p>or</p>

<pre><code>location ~ /WEB-INF/  { return 404; }
location ~ /META-INF/  { return 404; }
</code></pre>

<p>or</p>

<pre><code>location ~ /CFIDE/ {
    deny all;
    allow 127.0.0.1;
    }
</code></pre>

<p>Very few developers spend significant time configuring servers. Hence, it often may happen that the servers we maintain for our clients are not as secure as they could be. The simplicity with which we can secure a ColdFusion server behind Nginx seems a definite advantage.</p>

<p>However, the biggest advantage, to me, seems to be the ease of configuring strong SSL https security on Nginx. Because Nginx uses a reverse proxy setup, Nginx can be configured to handle the https connection with the requestor in isolation. What this implies is that SSL certificates do not need to be imported into the Java keystore. Nor do your SSL certs need to be reimported every time the JVM is updated. Of course, if you require a very secure setup, in a load balanced configuration to multiple application servers, you might need to ensure an SSL connection between Nginx and ColdFusion / Railo. But if, for instance, Nginx and ColdFusion / Railo are located on the same server, or you trust the internal network, this would not be necessary.</p>

<p>Following this <a href="https://raymii.org/s/tutorials/Strong_SSL_Security_On_nginx.html">excellent guide</a> by Remy van Elst, I was able to relatively easily setup SSL for a web application to an A+ level as graded by <a href="https://www.ssllabs.com/ssltest/index.html">Qualys SSL Labs</a>. Here&rsquo;s what the SSL portion of my Nginx configuration looks like:</p>

<pre><code>server {
    server_name  domain.com;
    listen  443 ssl;

    ssl_certificate     /etc/ssl/certs/domain_com.crt;
    ssl_certificate_key /etc/ssl/certs/domain_com.key;

    keepalive_timeout   70;
    ssl_ciphers         'AES256+EECDH:AES256+EDH:!aNULL';

    ssl_prefer_server_ciphers   on;
    ssl_dhparam         /etc/ssl/certs/dhparam.pem;

    ssl_session_cache shared:SSL:10m;
    ssl_protocols TLSv1 TLSv1.1 TLSv1.2;

    ssl_stapling on;
    ssl_stapling_verify on;
    resolver 8.8.8.8 8.8.4.4 valid=300s;
    resolver_timeout 5s;

    add_header Strict-Transport-Security max-age=345600;
    add_header X-Frame-Options DENY;
    add_header X-Content-Type-Options nosniff;

}
</code></pre>

<p>Note that I&rsquo;ve reduced the max-age value of the add_header Strict-Transport-Security directive to four days from what Remy has recommended, over uncertainty how this will play out when certificates are updated. I understand that this directive will force user agents to use https for the duration of the value set. Note that it will be updated into the future every time the header is sent to the browser.</p>

<p>There are 3 aspects of this configuration that require a bit of extra work on your part, besides simply copying, pasting and adjusting the above example. One is that if your SSL certificate provider has also issued intermediate and root certificates, you will need to concatenate these into one file. Your domain certificate must come first, followed by the intermediate certificates, and finally the root certificate at the end. The easiest way to do this is to create a blank text file, and copy and paste the contents of the certificates into it. Save it and upload it to the chosen location on your server, along with the private key. Point to the location of the concatenated cert in the Nginx config file using the ssl_certificate directive. Point to the location of the private key using the ssl_certificate_key directive.</p>

<p>On Linux, you should secure the key by running chmod 600 on the ssl directory that contains the certificate files and chmod 400 on the private key file itself.</p>

<p>To generate the file referred to in the ssl_dhparam directive in the above example, it is necessary to cd to your ssl certs directory and run the following command, as explained in <a href="https://raymii.org/s/tutorials/Strong_SSL_Security_On_nginx.html">Remy&rsquo;s guide</a> in the Forward Secrecy &amp; Diffie Hellman Ephemeral Parameters section:</p>

<pre><code>cd /etc/ssl/certs
openssl dhparam -out dhparam.pem 4096
</code></pre>

<p>You will be warned  that &ldquo;this will take awhile&rdquo;. Remy doesn&rsquo;t mention this, but be prepared for it to take a relatively long time, perhaps an hour or so.</p>

<p>Once you have ssl configured and working, you probably want to redirect any traffic trying to access the site via insecure http to secure https. Here&rsquo;s how to configure that:</p>

<pre><code>server {
    server_name  domain.com;
    listen  80;
    return  301 https://$server_name$request_uri;
}
</code></pre>

<p>I already mentioned what I think is the biggest advantage, the ease with which one can configure SSL. However, there is one other advantage I&rsquo;d like to mention that may trump that, the ability to easily configure Nginx for both load balancing and hot swapping of application servers. Here&rsquo;s how to do that using the upstream directive:</p>

<pre><code>upstream    cf_servers {
    #ip_hash;                   ## uncomment ip_hash for load balancing         
    server          127.0.0.1:8500;
    #server         192.168.0.20:8500;  ## add more application servers for load balancing
    keepalive       32;         ## number of upstream connections to keep alive
}

upstream    railo_servers {
    #ip_hash;                   ## uncomment ip_hash for load balancing         
    server          127.0.0.1:8888;
    #server         192.168.0.30:8888;  ## add more application servers for load balancing
    keepalive       32;         ## number of upstream connections to keep alive
}
</code></pre>

<p>Then instead of using the localhost IP address in your proxy_pass directive, use the upstream server name instead, like so:</p>

<pre><code>location / {
    proxy_pass  http://cf_servers/domain/;
}
</code></pre>

<p>If you get caught in a bind and can&rsquo;t use ColdFusion for some reason, then hot swapping an application over to Railo is as simply as changing the above to this:</p>

<pre><code>location / {
    proxy_pass  http://railo_servers/domain/;
}
</code></pre>

<p>Or if you find one of your web properties suddenly inundated with traffic, using the upstream directive, your webserver is already set up for load balancing. You&rsquo;d simply need to clone a few servers, boot them up, and add them to the upsteam directive. The ip_hash directive is a way to enable sticky sessions, to ensure a visitor is always routed to the same server.</p>

<p>A few more configuration options and details should be mentioned.</p>

<p>One is that it is also possible, and perhaps preferable, to configure the path to a website&rsquo;s files within Tomcat&rsquo;s server.xml file, rather than specifying it in the Nginx proxy_pass directive.</p>

<pre><code>&lt;Host name="domain.com" appBase="webapps"&gt;
    &lt;Context path="" docBase="/var/sites/domain.com" /&gt;
    &lt;Alias&gt;www.domain.com&lt;/Alias&gt;
&lt;/Host&gt;
</code></pre>

<p>With the path to the website&rsquo;s files configured in server.xml, the path to the website&rsquo;s files is not appended to the proxy_pass directive, so it would look like this:</p>

<pre><code>server {
    listen 80;
    server_name .domain.com;
    root /var/www/domain; 

    location ~* \.(?:ico|css|js|gif|jpe?g|png)$ {
        # specific settings for files ending in suffixes .ico, .css, .js, gif, etc
    }

    location / {
        proxy_pass  http://railo_servers/;
    }

}
</code></pre>

<p>For this to work, Nginx needs to pass the host name to the app server used, either Railo or ColdFusion. Your application may also require this information. Here is a suggested set of directives to accomplish this:</p>

<pre><code>    proxy_http_version  1.1;
    proxy_set_header    Connection "";
    proxy_set_header    Host                $host;
    proxy_set_header    X-Forwarded-Host    $host;
    proxy_set_header    X-Forwarded-Server  $host;
    proxy_set_header    X-Forwarded-For     $proxy_add_x_forwarded_for;     ## CGI.REMOTE_ADDR
    proxy_set_header    X-Forwarded-Proto   $scheme;                        ## CGI.SERVER_PORT_SECURE
    proxy_set_header    X-Real-IP           $remote_addr;
    expires             epoch;
</code></pre>

<p>&hellip; which could be placed in a separate conf file and included with each proxy_pass directive, like so:</p>

<pre><code>location / {
    proxy_pass  http://railo_servers/;
    include     /var/inc/nginx-cf-proxy-settings.conf;
}
</code></pre>

<p>If you are using Railo, adding a host configuration block to server.xml will cause Railo to create a WEB-INF directory under the docBase path specified along with an additional context. You&rsquo;d then want to block public access to that directory in some way as I&rsquo;ve explained above.</p>

<p>You can also use a regex to ensure you only pass CFML templates to Railo or ColdFusion, as the following example demonstrates:</p>

<pre><code>location ~ \.(cfm|cfml|cfc)$ {
    proxy_pass  http://railo_servers/;
    include     /var/inc/nginx-cf-proxy-settings.conf;
}
</code></pre>

<p>You can put whatever file extensions you&rsquo;d like Railo / ACF to process in the pipe delimited list within the regex, so <code>~ \.(cfm|cfml|cfc)$</code> or <code>~ \.(cfm|cfml|htm|html)$</code> could be used, for example.</p>

<p>A couple of small gotcha&rsquo;s I ran into setting the path to the website&rsquo;s files in the proxy_pass directive. One is that directories with dots in them can confuse mappings. The solution I used was to remove the dots from directory names and use hyphens instead ( my-domain-com ). I also had trouble with redirects in FW/1, since they rely on the value of cgi.script_name. The simple solution was to change the baseURL parameter of FW/1&rsquo;s config to <code>baseURL = ''</code> from the default <code>baseURL = 'useCgiScriptName'</code>. Problems such as these might be avoided by using the server.xml configuration option.</p>

<p>I have 2 production servers running Nginx at the moment, one ACF and one Railo server, and so far it seems to be working very well. I&rsquo;m particularly happy with the choice because Nginx has a very small memory footprint, which leaves more memory available for the JVM, which indeed yet another advantage.</p>

<p>References:</p>

<ol>
<li><a href="http://nginx.com/resources/admin-guide/web-server/">Nginx Web Server configuration guide</a></li>
<li><a href="https://raymii.org/s/tutorials/Strong_SSL_Security_On_nginx.html">Remy van Elst Strong SSL Security on Nginx</a></li>
<li><a href="https://gist.github.com/igal-getrailo/6981111">Igal&rsquo;s Nginx config example</a></li>
<li><a href="http://www.amazon.com/Mastering-Nginx-Dimitri-Aivaliotis/dp/1849517444">Mastering NGINX by Dimitri Aivaliotis</a></li>
</ol>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Exploring CentOS 7 Firewalld]]></title>
    <link href="http://dnando.github.io/blog/2014/12/02/exploring-centos-7-firewalld/"/>
    <updated>2014-12-02T10:41:04+01:00</updated>
    <id>http://dnando.github.io/blog/2014/12/02/exploring-centos-7-firewalld</id>
    <content type="html"><![CDATA[<p>New with CentOS 7 is firewalld, a replacement for iptables to manage the firewall. As with anything new, at first glance it seems confusing, but I&rsquo;m finding I prefer it over iptables.</p>

<p>The first thing to understand about firewalld is that it is has multiple layers. It comes with a predefined set of zones, namely block, dmz, drop, external, home, internal, public, trusted, and work. Each of those zones can be associated with a network device or one or more ip addresses. Essentially, zones define and demarcate the level of trust an admin has decided to place on the devices and traffic within a network.</p>

<p>firewalld also pre-defines a set of services that can be added or removed from a zone. Effectively, when a service is added to a zone, it opens a port and sets any other necessary parameters. Services are defined with XML. Here&rsquo;s what the http service looks like:
<code>
&lt;?xml version="1.0" encoding="utf-8"?&gt;
&lt;service&gt;
  &lt;short&gt;WWW (HTTP)&lt;/short&gt;
  &lt;description&gt;HTTP is the protocol used to serve Web pages. If you plan to make your Web server publicly available, enable this option. ... &lt;/description&gt;
  &lt;port protocol="tcp" port="80"/&gt;
&lt;/service&gt;
</code>
So to open port 80 via the tcp protocol to serve http requests, as an example, first a zone must be associated with the network device that will handle the traffic, and then the http service added to the zone. As an admin, you can define your own custom services, or customize existing services. Other techniques allow you to open a port directly on a zone, or define more complex rules for access.</p>

<p>To configure the firewall and check its status, a command line client is provided, firewall-cmd. It can be used to make both permanent and temporary config changes. The configuration for firewalld is stored in XML files in /usr/lib/firewalld/ ( the default settings, not to be modified! ) and /etc/firewalld/ ( for user configured settings, which have preference over those in the default location ). These files can be edited, backed up, or used as templates for other server installations.</p>

<p>Now that we have an overview, we can get to work. To check if firewalld is running:</p>

<pre><code>systemctl status firewalld
</code></pre>

<p>If you see from the output that firewalld is not running, or you see that the loaded service is disabled, here are the commands needed:</p>

<pre><code>systemctl enable firewalld
systemctl start firewalld
</code></pre>

<p>If a service is enabled, it will start on system reboot. Hence it&rsquo;s particularly important to ensure firewalld is enabled on a production server.</p>

<p>Here&rsquo;s how to disable firewalld so it will not start at boot time, and shut it down:</p>

<pre><code>systemctl disable firewalld
systemctl stop firewalld
</code></pre>

<p>Now I want to configure the firewall. First, I check for the name of the ethernet interface so that I can refer to it to associate it with a zone:</p>

<pre><code>nmcli dev status
</code></pre>

<p>Then I check which zone eno16777736 is currently assigned to:</p>

<pre><code>firewall-cmd --get-zone-of-interface=eno16777736
</code></pre>

<p>The result is &ldquo;no zone&rdquo;, so the next step is to add the ethernet interface to the public zone, which is the zone I&rsquo;ve decided to use for http access to the server. It&rsquo;s important to add the &ndash;permanent flag to the command so it is retained permanently, across reboots:</p>

<pre><code>firewall-cmd --zone=public --add-interface=eno16777736 --permanent
</code></pre>

<p>Now I have to reload the firewall configuration for the changes to take effect:</p>

<pre><code>firewall-cmd --reload
</code></pre>

<p>And then we can double check just to make sure the ethernet interface is now added to the public zone &hellip;</p>

<pre><code>firewall-cmd --get-zone-of-interface=eno16777736
</code></pre>

<p>and the result is &ldquo;public&rdquo;, so that&rsquo;s now set up correctly.</p>

<p>Let&rsquo;s now check how the public zone is currently set up:</p>

<pre><code>firewall-cmd --zone=public --list-all
</code></pre>

<p>Here we see again that the ethernet interface is added to the public zone, and that it is both active and the default zone. By default after installing CentOS 7, we have the services dhcpv6-client and ssh added to this zone. Taking a quick look at the description for this service to see what it does by opening /usr/lib/firewalld/services/dhcpv6-client.xml, we see, &ldquo;This option allows a DHCP for IPv6 (DHCPv6) client to obtain addresses and other IPv6 settings from DHCPv6 server.&rdquo; We won&rsquo;t be using IPv6 addresses within our local network to access this machine, so I think it&rsquo;s safe to remove this service, although we may want to leave it in place on a production server:</p>

<pre><code>firewall-cmd --zone=public --remove-service=dhcpv6-client --permanent
</code></pre>

<p><strong>Reminder - remember to always add the permanent flag to these commands if you want changes to be persisted!</strong></p>

<p>Now we can add the services for http access to our public zone:</p>

<pre><code>firewall-cmd --zone=public --add-service=http --permanent
firewall-cmd --zone=public --add-service=https --permanent
</code></pre>

<p>&hellip; reload the firewall &hellip;</p>

<pre><code>firewall-cmd --reload
</code></pre>

<p>&hellip; and recheck the configuration, using list-services instead of list-all just to try it out:</p>

<pre><code>firewall-cmd --zone=public --list-services
</code></pre>

<p>and I see that we now have services http https ssh configured. Excellent. Let&rsquo;s test that in a web browser.</p>

<p>I&rsquo;ve installed nginx web server, but see using systemctl status nginx that it&rsquo;s not yet running or enabled, so first we run</p>

<pre><code>systemctl start nginx
systemctl enable nginx
</code></pre>

<p>And then I go to 192.168.1.16 in a web browser and see <strong>Welcome to nginx!</strong> Good.</p>

<p>As a double check, let&rsquo;s remove the http service and see what happens.</p>

<pre><code>firewall-cmd --zone=public --remove-service=http --permanent
firewall-cmd --reload
</code></pre>

<p>Reloading 192.168.1.16, I get a No data received message, so that&rsquo;s exactly what we should expect.</p>

<pre><code>firewall-cmd --zone=public --add-service=http --permanent
firewall-cmd --reload
</code></pre>

<p>And adding the http service back to the public zone again allows the <strong>Welcome to nginx!</strong> page to be loaded in my browser. Perfect.</p>

<p>However, I still don&rsquo;t have access to the CF Admin panel at <a href="http://192.168.1.16:8500/CFIDE/administrator/index.cfm,">http://192.168.1.16:8500/CFIDE/administrator/index.cfm,</a> because that&rsquo;s over port 8500. On a production machine, I absolutely would not open port 8500 for this purpose. But since this server is on our local office network, let&rsquo;s see how we can do this.</p>

<p>The first option that comes to mind is to create a custom firewalld service specifically for this purpose. Documentation I&rsquo;ve read recommends using existing services as a template. Custom services go in /etc/firewalld/services/. First let&rsquo;s make a copy of the http service, calling it http8500, and place it in /etc/firewalld/services/:</p>

<pre><code>cp /usr/lib/firewalld/services/http.xml /etc/firewalld/services/http8500.xml
</code></pre>

<p>Then we edit /etc/firewalld/services/http8500.xml to use port 8500 instead of port 80. Here&rsquo;s what the modified file looks like:</p>

<pre><code>&lt;?xml version="1.0" encoding="utf-8"?&gt;
&lt;service&gt;
  &lt;short&gt;CF Admin Access&lt;/short&gt;
  &lt;description&gt;For CF Admin access via port 8500.&lt;/description&gt;
  &lt;port protocol="tcp" port="8500"/&gt;
&lt;/service&gt;
</code></pre>

<p>Then we add this service to the public zone and reload the firewall:</p>

<pre><code>firewall-cmd --zone=public --add-service=http8500 --permanent
firewall-cmd --reload
</code></pre>

<p>And now <a href="http://192.168.1.16:8500/CFIDE/administrator/index.cfm">http://192.168.1.16:8500/CFIDE/administrator/index.cfm</a> works! Again, this is not how I&rsquo;d set up access to the CF administrator on a production machine, but it was an opportunity to experiment with creating custom services. What I like about this option is that I can enable or disable it, independently of the other services enabled. So if I decide I want to lock this server down, I can quickly remove the http8500 service and access the CF Administrator via <a href="http://dnando.github.io/blog/2014/11/04/ssh-tunneling-coldfusion-lockdown-technique/">SSH Tunnelling</a>.</p>

<p>What I usually do is move ssh access to an obscure port. I think we can easily accomplish this using a custom service, but before I do that, I want to take a look at how the localhost interface is set up within the firewall. Again, we use nmcli dev status to get the name of the localhost or loopback interface</p>

<pre><code>nmcli dev status
</code></pre>

<p>It&rsquo;s &ldquo;lo&rdquo;, so let&rsquo;s see if it&rsquo;s set to zone by default:</p>

<pre><code>firewall-cmd --get-zone-of-interface=lo
</code></pre>

<p>Nope, the result I get is &ldquo;no zone&rdquo;. Let&rsquo;s also see if there are any services added to the trusted zone, which would be the most appropriate for localhost</p>

<pre><code>firewall-cmd --zone=trusted --list-all
</code></pre>

<p>At this point, nothing is added to this zone, no interfaces, services, sources or ports, etc. And the network interface &ldquo;lo&rdquo; isn&rsquo;t associated with any zone.</p>

<p>Now what I want to see is how the server responds to localhost access with the firewall enabled. This <em>might</em> be important on a production server because I will use ssh tunneling to access any areas I will restrict from public access. So let&rsquo;s logout from the server and login again with the -D flag so that I can tunnel into the test server and test if I have access via localhost with the firewall setup as it is now:</p>

<pre><code>exit
ssh -D 6100 root@192.168.1.16
root@192.168.1.16's password:
</code></pre>

<p>I keep Firefox on my dev machine reserved and set up for ssh tunneling on port 6100, so I simply open Firefox and browse to <a href="http://localhost:8500/CFIDE/administrator/index.cfm,">http://localhost:8500/CFIDE/administrator/index.cfm,</a> and find I can access the CF11 admin page and login. Browsing to <a href="http://localhost/,">http://localhost/,</a> I see the Welcome to nginx! page. So at this point via localhost, I have access. ( Note for anyone without experience using ssh tunnelling,  when I use localhost on Firefox set up for ssh tunneling, logged to the CentOS server using the -D flag, I am browsing the CentOS server next to me, not my dev machine. See <a href="http://dnando.github.io/blog/2014/11/04/ssh-tunneling-coldfusion-lockdown-technique/">SSH Tunnelling</a> for details how to do this. )</p>

<p>Now what happens if I add the &ldquo;lo&rdquo; network interface to the trusted zone, where no access is currently set up?</p>

<pre><code>firewall-cmd --zone=trusted --add-interface=lo --permanent
firewall-cmd --reload
</code></pre>

<p>Adding the &ldquo;lo&rdquo; interface to the trusted zone with no services had no affect at all on tunnelled access to localhost. So it looks like the firewall doesn&rsquo;t interfere there. So to clean up, I will remove the &ldquo;lo&rdquo; interface from the trusted zone and call it a day.</p>

<pre><code>firewall-cmd --zone=trusted --remove-interface=lo --permanent
firewall-cmd --reload
</code></pre>

<p>PS - For some reason, &ldquo;lo&rdquo; was not removed from the trusted zone according to firewall-cmd &ndash;zone=trusted &ndash;list-all unless and until I rebooted the server. The strange thing was that the config file was correctly altered, but somehow, firewalld didn&rsquo;t seem to pick up the change. Perhaps this is the intended behavior, to prevent lockout during a current session, but I&rsquo;ll look into filing a bug report later this evening &hellip; ( which I have now filed <a href="https://bugs.centos.org/view.php?id=7957">here</a> ).</p>

<p>References:</p>

<ol>
<li><a href="https://access.redhat.com/documentation/en-US/Red_Hat_Enterprise_Linux/7/html/Security_Guide/sec-Using_Firewalls.html">https://access.redhat.com/documentation/en-US/Red_Hat_Enterprise_Linux/7/html/Security_Guide/sec-Using_Firewalls.html</a></li>
<li><a href="http://www.certdepot.net/rhel7-get-started-firewalld/">http://www.certdepot.net/rhel7-get-started-firewalld/</a></li>
<li><a href="http://fedoraproject.org/wiki/FirewallD">http://fedoraproject.org/wiki/FirewallD</a></li>
</ol>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Setting Up a Local Testing Server on VMWare]]></title>
    <link href="http://dnando.github.io/blog/2014/12/01/setting-up-a-local-testing-server-on-vmware/"/>
    <updated>2014-12-01T15:55:49+01:00</updated>
    <id>http://dnando.github.io/blog/2014/12/01/setting-up-a-local-testing-server-on-vmware</id>
    <content type="html"><![CDATA[<p>I&rsquo;ve been wondering how to best set up a local testing environment for some time. My production servers are all on CentOS. On the advice of Nolan Erck, whom I met at CFCamp this year, I&rsquo;ve decided to use VMWare to set up a CentOS server on a virtual machine here in the office. This approach allows me to have multiple test environments, and I can easily move them to another machine if needed simply by copying the disk image.</p>

<p>For ease of access, each VM needs a static ip on the local network. It&rsquo;s not hard to do once you figure it out, but getting all the pieces in place took some hours of digging, trial and error. This post, while specific to VMWare and CentOS 7, is intended to help both my future self and anyone else set up networking quickly and easily in such a scenario.  Adapt as necessary for your specific environment.</p>

<p>The obvious first step was to download the ISO image from a <a href="http://www.centos.org/download/mirrors/">CentOS mirror site</a>. I grabbed the minimal install as a nearest approximation to a production environment, and created a VM from it using VMWare on an unused Mac mini we had laying around the office.</p>

<p>On the minimal install, networking isn&rsquo;t enabled by default. So before I could proceed, I had to figure how to enable networking, and get it working via a static IP. Here&rsquo;s a summary of what finally worked for me.</p>

<p>1) Figure out what the ethernet device is named by running the command :</p>

<pre><code>nmcli dev status
</code></pre>

<p><img src="/images/nmcli.gif" alt="nmcli dev status" /></p>

<p>As you can see, mine was named eno16777736, which is not the RedHat default you may find in many examples online.</p>

<p>2) cd to the directory /etc/sysconfig/network-scripts and run ls to display its files</p>

<pre><code>cd /etc/sysconfig/network-scripts
ls
</code></pre>

<p><img src="/images/networkScriptsLs.gif" alt="network-scripts ls" /></p>

<p>3) Look for the configuration file for your ethernet device, mine was named ifcfg-eno16777736 and open it for editing using vm or nano</p>

<pre><code>nano ifcfg-eno16777736
</code></pre>

<p><img src="/images/nano-ifcfg.gif" alt="network-scripts ls" /></p>

<p>The above screenshot was taken after it was edited. The lines to change or add are:
<code>
BOOTPROTO=static
IPADDR=&lt;the static ip address you want to assign to this instance&gt;
NETMASK=255.255.255.0
GATEWAY=&lt;the gateway ip address of your internal network&gt;
NM_CONTROLLED=no
ONBOOT=yes
</code>
I found that adding the correct gateway ip was essential. NM_CONTROLLED specifies whether or not this device is controlled by the Network Manager. We are setting the parameters manually here, so this must be no. ONBOOT=yes specifies to connect this network device on boot.</p>

<p>Save the file, exit nano, and run the following command to restart the network:</p>

<pre><code>systemctl restart network
</code></pre>

<p>Now CentOS 7 should be setup to network via the static local ip of your choice. But the connection isn&rsquo;t bridged outside of the VM. After some fiddling around, here&rsquo;s what worked for me. I went out to the VMWare interface, clicked on the double arrow icon to open the networking menu, and clicked Network Adapter Settings&hellip;</p>

<p><img src="/images/vmWareNAS1.jpg" alt="VMWare Network menu" /></p>

<p>From the menu, I chose Autodetect, as shown below:</p>

<p><img src="/images/vmWareNAS2.jpg" alt="VMWare Network menu" /></p>

<p>Once I had these configuration changes in place, I could access the CentOS instance via SSH and SFTP from my dev machine.</p>

<p>By the way, from my reading, it also seems possible to use the Network Manager to achieve the same end. In this case, you&rsquo;d leave the config file for your network device alone, and instead run the nmtui command. Search Google for more complete instructions. You&rsquo;ll still need to bridge the connection through VMWare tho&#8217;.</p>

<p>Hope this helps somebody.</p>

<p>PS - If you can access the instance via SSH or SFTP, but cannot from a browser, you may need to either disable and stop firewalld:</p>

<pre><code>systemctl disable firewalld
systemctl stop firewalld
</code></pre>

<p>or better, figure out how to configure it properly to allow access via a browser, which I cover in the next article, <a href="http://dnando.github.io/blog/2014/12/02/exploring-centos-7-firewalld/">Exploring CentOS 7 firewalld</a></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[How to Rollback Yum Updates on CentOS]]></title>
    <link href="http://dnando.github.io/blog/2014/10/24/how-to-rollback-yum-updates-on-centos/"/>
    <updated>2014-10-24T16:26:50+02:00</updated>
    <id>http://dnando.github.io/blog/2014/10/24/how-to-rollback-yum-updates-on-centos</id>
    <content type="html"><![CDATA[<p>1) Open the yum config file:</p>

<pre><code>vi /etc/yum.conf
</code></pre>

<p>and ensure the following line is included in the config settings</p>

<pre><code>tsflags=repackage
</code></pre>

<p>2) Open the rpm macro file ( or create it if it doesn&rsquo;t exist ):</p>

<pre><code>vi /etc/rpm/macros
</code></pre>

<p>And ensure the following line is included:</p>

<pre><code>%_repackage_all_erasures 1
</code></pre>

<p>Now the rollback flag can be used on the rpm command as shown in the examples below to roll back updates to any point in the past:</p>

<pre><code>rpm -Uvh –rollback ’21:00′
rpm -Uvh –rollback ’3 hours ago’
rpm -Uvh –rollback ‘august 13′
rpm -Uvh –rollback ‘yesterday’
</code></pre>

<p>Simple.</p>
]]></content>
  </entry>
  
</feed>
